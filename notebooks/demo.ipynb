{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Auto-Agent Designer â€” Demo Notebook\n",
    "This notebook demonstrates the full Day-1 pipeline:\n",
    "- Load mission\n",
    "- Generate AgentSpecs using MetaAgentGenerator\n",
    "- Simulate them\n",
    "- Evaluate mission success\n",
    "- Display leaderboard\n",
    "\n",
    "You can run this notebook in **Colab**, **Kaggle**, or locally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))  # Ensure src is importable"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Install dependencies (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install pyyaml rich python-dotenv pytest"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Load MetaAgentGenerator and Mission File"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.meta_agent.generator import MetaAgentGenerator\n",
    "\n",
    "mission_path = \"../data/missions.yml\"\n",
    "\n",
    "mg = MetaAgentGenerator()\n",
    "specs = mg.run_from_file(mission_path)\n",
    "\n",
    "specs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Run Simulator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.eval.simulator import run_simulator\n",
    "\n",
    "results = run_simulator(specs)\n",
    "results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ† Display Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.eval.leaderboard import pretty_print\n",
    "pretty_print(results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Evaluate Mission Success"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.eval.evaluator import Evaluator\n",
    "ev = Evaluator()\n",
    "ev.evaluate(results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Done!\n",
    "You now have:\n",
    "\n",
    "âœ” Agent generation  
    " ,
    "âœ” Simulation  
    ",
    "âœ” Leaderboard  
    ",
    "âœ” Metrics\n",
    "\n",
    "Your Auto-Agent Designer MVP is fully operational. ğŸš€"
   ]
  }
 ]
} 
